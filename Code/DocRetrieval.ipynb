{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85ea11c-13e9-4546-8461-48ba788b521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell doesn't depend on the previous ones\n",
    "# It can run by itself!\n",
    "\n",
    "# Enter your code here\n",
    "# Load the SGNews Corpus\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "speechCorpus = PlaintextCorpusReader('Dataset', '.+\\.txt')\n",
    "speechFids = speechCorpus.fileids()\n",
    "# print(speechFids)\n",
    "\n",
    "# For each file ID in fids, the code below takes its words. All these files are\n",
    "# then combined into a single list called 'docs'.\n",
    "speechDocs = [speechCorpus.words(f) for f in speechCorpus.fileids()]\n",
    "\n",
    "# Convert words to lower case\n",
    "speechLowered = [[w.lower() for w in doc] for doc in speechDocs]\n",
    "\n",
    "# Use regular expression to keep only alphabetic words.\n",
    "import re\n",
    "speechAlpha = [[w for w in doc if re.search('^[a-z]+$|377a',w)] for doc in speechLowered]\n",
    "\n",
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopList = stopwords.words('english')\n",
    "speechStopped = [[w for w in doc if w not in stopList] for doc in speechAlpha]\n",
    "\n",
    "# Stem the words: Porter stemming.\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "speechStemmed = [[stemmer.stem(w) for w in doc] for doc in speechStopped]\n",
    "\n",
    "# Create a dictionary\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "speechDictionary = corpora.Dictionary(speechStemmed)\n",
    "# print(speechDictionary)\n",
    "\n",
    "# Convert documents to vectors\n",
    "speechVectors = [speechDictionary.doc2bow(doc) for doc in speechStemmed]\n",
    "\n",
    "# To find the similarity scores, create a reverse index\n",
    "from gensim import similarities\n",
    "speechIndex = similarities.SparseMatrixSimilarity(speechVectors, len(speechDictionary))\n",
    "\n",
    "# Create a TFIDF reveset index\n",
    "from gensim import models\n",
    "speechTFIDF = models.TfidfModel(speechVectors)\n",
    "speechVectorsWithTFIDF = [speechTFIDF[vec] for vec in speechVectors]\n",
    "speechIndexWithTFIDF = similarities.SparseMatrixSimilarity(speechVectorsWithTFIDF, len(speechDictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc7f3604-87e9-4064-aac8-2c6e9501a37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results using RAW scores. (Filename, score):\n",
      "('2015_chi.txt', 0.053266563)\n",
      "('2016_eng.txt', 0.030846732)\n",
      "('2015_eng.txt', 0.023017528)\n",
      "('2006_eng.txt', 0.018219689)\n",
      "('2010_mal.txt', 0.015515535)\n",
      "\n",
      "Search results using TFIDF scores. (Filename, score):\n",
      "('2015_chi.txt', 0.06582709)\n",
      "('2015_eng.txt', 0.053857617)\n",
      "('2016_eng.txt', 0.039161)\n",
      "('2006_eng.txt', 0.017614689)\n",
      "('2014_eng.txt', 0.013898119)\n"
     ]
    }
   ],
   "source": [
    "# A Query string\n",
    "query = \"haze from indonesia\"\n",
    "qList = query.split()\n",
    "qLower = [w.lower() for w in qList]\n",
    "\n",
    "# Stem it (no need to use stop words <= will not be there in the index any way)\n",
    "qStemmed = [stemmer.stem(w) for w in qLower]\n",
    "\n",
    "# Create a query vector using the same dicitonary as the corpus\n",
    "qVector = speechDictionary.doc2bow(qStemmed)\n",
    "\n",
    "# Get its TFIDF from the same model as the corpus\n",
    "qVectorTFIDF = speechTFIDF[qVector]\n",
    "# print(qVector, qVectorTFIDF)\n",
    "\n",
    "# Get the similaries from the two indexes (raw and TFIDF)\n",
    "simRaw = speechIndex[qVector]\n",
    "simTFIDF = speechIndexWithTFIDF[qVectorTFIDF]\n",
    "\n",
    "# sort them\n",
    "simSorted = sorted(enumerate(simRaw), key = lambda item: -item[1])\n",
    "simTFIDFSorted = sorted(enumerate(simTFIDF), key = lambda item: -item[1])\n",
    "\n",
    "# print(\"Raw search scores: \", simSorted[0:5])\n",
    "# print(\"TFIDF search scores: \", simTFIDFSorted[0:5])\n",
    "\n",
    "# Translate the file ids to filenames\n",
    "# In the list comprehension, sim is a tuple, and its elements are extracted just like a list\n",
    "# Note that the output also is a list of tuples, with the first element replaced by the filename\n",
    "fRaw = [(speechCorpus.fileids()[sim[0]], sim[1]) for sim in simSorted[0:5]]\n",
    "fTFIDF = [(speechCorpus.fileids()[sim[0]], sim[1]) for sim in simTFIDFSorted[0:5]]\n",
    "\n",
    "# Using the * operator, you can use the separator\n",
    "# See https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/\n",
    "print(\"Search results using RAW scores. (Filename, score):\")\n",
    "print(*fRaw, sep = '\\n')\n",
    "print(\"\\nSearch results using TFIDF scores. (Filename, score):\")\n",
    "print(*fTFIDF, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251746bc-5832-4985-8692-6675eb8d89a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
